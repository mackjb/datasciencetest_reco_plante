# ğŸš€ Guide de DÃ©marrage - Dev Container GPU RTX 5070

## ğŸ“ Vue d'ensemble

Ce dev container vous permet d'utiliser votre **GPU NVIDIA RTX 5070** avec **CUDA 12.8** pour vos projets de Deep Learning avec TensorFlow et PyTorch.

---

## âš¡ DÃ©marrage rapide (3 Ã©tapes)

### 1ï¸âƒ£ VÃ©rifier les prÃ©requis

**Sur Windows, ouvrez PowerShell et exÃ©cutez :**

```powershell
# VÃ©rifier le driver NVIDIA
nvidia-smi

# VÃ©rifier Docker GPU
docker run --rm --gpus all nvidia/cuda:12.8.0-base-ubuntu24.04 nvidia-smi
```

âœ… **RÃ©sultat attendu :** Vous devez voir les informations de votre GPU RTX 5070.

âŒ **Si erreur :** Installez les drivers NVIDIA >= 570.x et Docker Desktop avec WSL2.

---

### 2ï¸âƒ£ Ouvrir le projet dans VS Code

**MÃ©thode A : Via VS Code (recommandÃ©)**

1. Ouvrir VS Code dans le dossier du projet
2. Appuyer sur `F1`
3. Taper et sÃ©lectionner : **"Dev Containers: Reopen in Container"**
4. Attendre 5-10 minutes (premiÃ¨re fois seulement)

**MÃ©thode B : Via PowerShell**

```powershell
cd c:\repository\datascience_rpojet_DS_2\.devcontainer
.\dev-commands.ps1 build
.\dev-commands.ps1 run
```

---

### 3ï¸âƒ£ VÃ©rifier que le GPU fonctionne

**Une fois dans le container :**

```bash
# Test complet
python .devcontainer/test_gpu.py

# Ou test rapide
python -c "import tensorflow as tf; print('GPUs:', tf.config.list_physical_devices('GPU'))"
python -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

âœ… **RÃ©sultat attendu :** Les GPUs sont dÃ©tectÃ©s par TensorFlow et PyTorch.

---

## ğŸ“š Utilisation quotidienne

### ğŸ“ Lancer Jupyter Lab

```bash
jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root
```

Puis ouvrir dans votre navigateur : **http://localhost:8888**

---

### ğŸ“Š Lancer TensorBoard

```bash
tensorboard --logdir=./logs --host=0.0.0.0 --port=6006
```

Puis ouvrir : **http://localhost:6006**

---

### ğŸ” Surveiller le GPU

```bash
# Monitoring en temps rÃ©el
watch -n 2 nvidia-smi

# Ou une seule fois
nvidia-smi
```

---

## ğŸ§ª Exemples de code

### TensorFlow GPU

```python
import tensorflow as tf

# VÃ©rifier les GPUs
print("GPUs disponibles:", len(tf.config.list_physical_devices('GPU')))

# Calcul sur GPU
with tf.device('/GPU:0'):
    a = tf.random.normal([1000, 1000])
    b = tf.random.normal([1000, 1000])
    c = tf.matmul(a, b)
    print(f"RÃ©sultat: {c.shape}")
```

### PyTorch GPU

```python
import torch

# VÃ©rifier CUDA
print("CUDA disponible:", torch.cuda.is_available())
print("GPU:", torch.cuda.get_device_name(0))

# Calcul sur GPU
device = torch.device('cuda')
a = torch.randn(1000, 1000, device=device)
b = torch.randn(1000, 1000, device=device)
c = torch.matmul(a, b)
print(f"RÃ©sultat: {c.shape}, Device: {c.device}")
```

---

## ğŸ“¦ Gestion des packages

### âœ… FAIRE : Ajouter via Conda

1. Modifier `.devcontainer/environment.yml`
2. Ajouter votre package :
   ```yaml
   dependencies:
     - votre-package>=version
   ```
3. Reconstruire le container :
   - Dans VS Code : `F1` â†’ "Dev Containers: Rebuild Container"
   - Ou : `.\dev-commands.ps1 rebuild`

### âŒ NE PAS FAIRE : pip install en systÃ¨me

```bash
pip install package  # âŒ VIOLE PEP 668
```

**Pourquoi ?** Ubuntu 24.04 protÃ¨ge l'environnement systÃ¨me Python. Utilisez **toujours Conda**.

---

## ğŸ› ï¸ Commandes PowerShell

Script d'aide : `.devcontainer\dev-commands.ps1`

```powershell
# Construire l'image
.\dev-commands.ps1 build

# Lancer le container
.\dev-commands.ps1 run

# Ouvrir un shell
.\dev-commands.ps1 shell

# Tester le GPU
.\dev-commands.ps1 test-gpu

# Lancer Jupyter
.\dev-commands.ps1 jupyter

# VÃ©rifier GPU hÃ´te
.\dev-commands.ps1 check-gpu

# Nettoyer Docker
.\dev-commands.ps1 clean

# Rebuild complet
.\dev-commands.ps1 rebuild

# Aide
.\dev-commands.ps1 help
```

---

## ğŸ› ProblÃ¨mes courants

### â“ GPU non dÃ©tectÃ© dans le container

**Diagnostic :**
```bash
# Dans le container
nvidia-smi
```

**Solutions :**
1. VÃ©rifier driver NVIDIA sur Windows : `nvidia-smi`
2. VÃ©rifier Docker GPU : `docker run --rm --gpus all nvidia/cuda:12.8.0-base-ubuntu24.04 nvidia-smi`
3. Reconstruire le container : `F1` â†’ "Rebuild Container"
4. VÃ©rifier `devcontainer.json` contient `"--gpus=all"` dans `runArgs`

---

### â“ Erreur "Out of Memory" GPU

**Solutions :**
```python
# TensorFlow - Activer croissance mÃ©moire (dÃ©jÃ  configurÃ©)
import os
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

# PyTorch - Vider le cache
import torch
torch.cuda.empty_cache()

# RÃ©duire la taille des batchs
batch_size = 16  # Au lieu de 32 ou 64
```

---

### â“ Container lent au premier dÃ©marrage

**Normal !** Le premier build prend 5-10 minutes car il :
- TÃ©lÃ©charge l'image CUDA (~2 GB)
- Installe Mambaforge
- Installe TensorFlow, PyTorch et tous les packages

Les dÃ©marrages suivants sont quasi instantanÃ©s.

---

### â“ TensorFlow affiche des warnings CUDA

**Exemples de warnings normaux (pas d'erreurs) :**
```
PTX JIT compilation...
Could not find cuda drivers...
```

**Ces warnings sont OK** tant que `tf.config.list_physical_devices('GPU')` dÃ©tecte le GPU.

Le cache PTX est montÃ© dans `~/.nv` pour accÃ©lÃ©rer les compilations suivantes.

---

## ğŸ“ Structure des fichiers

```
.devcontainer/
â”œâ”€â”€ Dockerfile              # Image Docker CUDA 12.8
â”œâ”€â”€ devcontainer.json       # Config VS Code + GPU
â”œâ”€â”€ environment.yml         # Packages Conda/pip
â”œâ”€â”€ test_gpu.py            # Script de test GPU
â”œâ”€â”€ example_gpu_test.ipynb # Notebook de dÃ©mo
â”œâ”€â”€ dev-commands.ps1       # Script PowerShell
â”œâ”€â”€ quick_start.sh         # Script bash de dÃ©marrage
â”œâ”€â”€ README.md              # Documentation complÃ¨te
â”œâ”€â”€ QUICK_REFERENCE.md     # RÃ©fÃ©rence rapide
â”œâ”€â”€ GETTING_STARTED.md     # Ce fichier
â””â”€â”€ .dockerignore          # Fichiers Ã  ignorer
```

---

## ğŸ¯ Checklist de validation

Avant de commencer vos projets, vÃ©rifiez :

- [ ] `nvidia-smi` affiche le RTX 5070
- [ ] `tf.config.list_physical_devices('GPU')` retourne 1+ GPU
- [ ] `torch.cuda.is_available()` retourne `True`
- [ ] Test matriciel GPU rÃ©ussi (plus rapide que CPU)
- [ ] Jupyter Lab accessible sur http://localhost:8888
- [ ] Environnement Conda `gpu-env` activÃ©
- [ ] Pas d'erreur PEP 668

---

## ğŸ“ Ressources et documentation

### Documentation locale
- **Guide complet** : `.devcontainer/README.md`
- **RÃ©fÃ©rence rapide** : `.devcontainer/QUICK_REFERENCE.md`
- **Notebook exemple** : `.devcontainer/example_gpu_test.ipynb`

### Documentation externe
- [NVIDIA CUDA Docs](https://docs.nvidia.com/cuda/)
- [TensorFlow GPU Guide](https://www.tensorflow.org/install/gpu)
- [PyTorch CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html)
- [Dev Containers](https://code.visualstudio.com/docs/devcontainers/containers)

---

## ğŸ’¡ Conseils et bonnes pratiques

### Performance GPU

1. **Batch size** : Augmentez pour mieux utiliser le GPU (32, 64, 128)
2. **Mixed Precision** : Utilisez FP16 pour plus de vitesse
   ```python
   # TensorFlow
   from tensorflow.keras import mixed_precision
   mixed_precision.set_global_policy('mixed_float16')
   
   # PyTorch
   from torch.cuda.amp import autocast
   with autocast():
       output = model(input)
   ```
3. **PrÃ©chargement donnÃ©es** : Utilisez `prefetch` pour Ã©viter d'attendre le GPU
   ```python
   # TensorFlow
   dataset = dataset.prefetch(tf.data.AUTOTUNE)
   ```

### Organisation du code

1. **Versionnez vos expÃ©riences** : Utilisez MLflow ou W&B
2. **Sauvegardez rÃ©guliÃ¨rement** : Checkpoints tous les N epochs
3. **Utilisez TensorBoard** : Monitoring en temps rÃ©el

### SÃ©curitÃ©

1. **API Keys** : Utilisez `.env` (ajoutez dans `.gitignore`)
2. **DonnÃ©es sensibles** : Ne committez jamais les datasets
3. **ModÃ¨les entraÃ®nÃ©s** : Stockez en externe (pas dans Git)

---

## ğŸ†˜ Support

### En cas de problÃ¨me

1. **Consultez README.md** : Documentation dÃ©taillÃ©e
2. **VÃ©rifiez QUICK_REFERENCE.md** : Solutions rapides
3. **Testez** : `python .devcontainer/test_gpu.py`
4. **Logs** : `.\dev-commands.ps1 logs`

### Informations utiles pour le debug

```bash
# Version Python
python --version

# Packages installÃ©s
conda list

# Info GPU
nvidia-smi

# Info Docker
docker --version
docker info | grep -i runtime
```

---

## âœ… PrÃªt Ã  commencer !

Votre environnement GPU est maintenant configurÃ© et prÃªt pour :

- ğŸ§  **Deep Learning** : TensorFlow, PyTorch, Keras
- ğŸ–¼ï¸ **Computer Vision** : OpenCV, PIL, ImageIO
- ğŸ“Š **Data Science** : NumPy, Pandas, Scikit-learn
- ğŸ“ˆ **Visualisation** : Matplotlib, Seaborn, Plotly
- ğŸ¤— **NLP** : HuggingFace Transformers
- ğŸš€ **MLOps** : TensorBoard, MLflow, W&B

**Bon coding ! ğŸ‰**

---

**Configuration** : NVIDIA RTX 5070 | CUDA 12.8.0 | TensorFlow â‰¥2.20 | PyTorch â‰¥2.5 | Python 3.12
