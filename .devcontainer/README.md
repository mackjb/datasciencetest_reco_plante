# Dev Container GPU RTX 5070 - CUDA 12.8

Configuration complÃ¨te pour utiliser TensorFlow/PyTorch sur GPU RTX 5070 avec CUDA 12.8.

## ðŸ“‹ PrÃ©requis

### Sur la machine hÃ´te (Windows)

1. **Drivers NVIDIA** : Version compatible avec CUDA 12.8
   ```powershell
   nvidia-smi  # VÃ©rifier la version du driver
   ```
   - Driver requis : >= 570.x pour CUDA 12.8

2. **Docker Desktop** avec support GPU
   - Installer Docker Desktop pour Windows
   - Activer l'intÃ©gration WSL2
   - VÃ©rifier le support GPU :
     ```powershell
     docker run --rm --gpus all nvidia/cuda:12.8.0-base-ubuntu24.04 nvidia-smi
     ```

3. **VS Code** avec l'extension Dev Containers
   - Installer VS Code
   - Installer l'extension "Dev Containers" (ms-vscode-remote.remote-containers)

## ðŸš€ Utilisation

### MÃ©thode 1 : Ouvrir dans VS Code

1. Ouvrir le projet dans VS Code
2. Appuyer sur `F1` ou `Ctrl+Shift+P`
3. SÃ©lectionner : **"Dev Containers: Reopen in Container"**
4. Attendre la construction du container (5-10 minutes la premiÃ¨re fois)
5. Le container s'ouvrira automatiquement avec l'environnement GPU activÃ©

### MÃ©thode 2 : Ligne de commande

```bash
# Depuis le rÃ©pertoire du projet
cd c:/repository/datascience_rpojet_DS_2

# Construire le container
docker build -t gpu-dev-env .devcontainer/

# Lancer le container
docker run -it --gpus all --shm-size=4g \
  -v ${PWD}:/workspace \
  -v ~/.nv:/root/.nv \
  -p 8888:8888 -p 6006:6006 \
  gpu-dev-env
```

## ðŸ§ª VÃ©rification GPU

### Test automatique

Le script `test_gpu.py` s'exÃ©cute automatiquement au dÃ©marrage du container.
Pour le relancer manuellement :

```bash
python /workspace/.devcontainer/test_gpu.py
```

### Tests manuels

#### TensorFlow
```python
import tensorflow as tf

# VÃ©rifier la version et les GPUs
print(f"TensorFlow version: {tf.__version__}")
print(f"GPUs disponibles: {len(tf.config.list_physical_devices('GPU'))}")
print(f"GPU dÃ©tails: {tf.config.list_physical_devices('GPU')}")

# Test de calcul
with tf.device('/GPU:0'):
    a = tf.random.normal([1000, 1000])
    b = tf.random.normal([1000, 1000])
    c = tf.matmul(a, b)
    print(f"Calcul rÃ©ussi: {c.shape}")
```

#### PyTorch
```python
import torch

# VÃ©rifier CUDA
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"GPU count: {torch.cuda.device_count()}")
print(f"GPU name: {torch.cuda.get_device_name(0)}")

# Test de calcul
device = torch.device('cuda')
a = torch.randn(1000, 1000, device=device)
b = torch.randn(1000, 1000, device=device)
c = torch.matmul(a, b)
print(f"Calcul rÃ©ussi: {c.shape}, Device: {c.device}")
```

## ðŸ“¦ Packages installÃ©s

### Deep Learning
- **TensorFlow** >= 2.20 (avec support CUDA 12.8)
- **PyTorch** >= 2.5 (avec torchvision, torchaudio)
- **Keras** >= 3.0
- **TensorRT** >= 8.6 (optimisation infÃ©rence)

### Data Science
- **NumPy**, **Pandas**, **Scikit-learn**
- **Matplotlib**, **Seaborn**, **Plotly**
- **OpenCV**, **Pillow**, **ImageIO**

### Outils ML
- **Jupyter Lab** 4.0+
- **TensorBoard**
- **Weights & Biases** (wandb)
- **MLflow**
- **HuggingFace** (transformers, datasets, accelerate)

## ðŸ”§ Configuration

### Variables d'environnement

Les variables suivantes sont configurÃ©es automatiquement :

- `NVIDIA_VISIBLE_DEVICES=all` : Tous les GPUs visibles
- `NVIDIA_DRIVER_CAPABILITIES=compute,utility` : CapacitÃ©s nÃ©cessaires
- `TF_FORCE_GPU_ALLOW_GROWTH=true` : TensorFlow alloue la mÃ©moire Ã  la demande
- `CUDA_CACHE_PATH=/root/.nv/ComputeCache` : Cache pour Ã©viter la recompilation PTX

### Montages de volumes

- **Workspace** : `${workspaceFolder}` â†’ `/workspace`
- **Cache GPU** : `~/.nv` â†’ `/root/.nv` (Ã©vite la recompilation JIT PTX)
- **Cache packages** : `~/.cache` â†’ `/root/.cache` (accÃ©lÃ¨re les installations)

### Ports exposÃ©s

- **8888** : Jupyter Lab
- **6006** : TensorBoard

## ðŸŽ¯ Lancer Jupyter Lab

```bash
# Dans le container
jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root
```

Puis ouvrir le lien affichÃ© dans le terminal (http://127.0.0.1:8888/lab?token=...)

## ðŸ› DÃ©pannage

### ProblÃ¨me : GPU non dÃ©tectÃ©

**VÃ©rifications :**
1. Driver NVIDIA installÃ© et Ã  jour sur l'hÃ´te
   ```bash
   nvidia-smi  # Sur l'hÃ´te Windows
   ```

2. Docker a accÃ¨s au GPU
   ```bash
   docker run --rm --gpus all nvidia/cuda:12.8.0-base-ubuntu24.04 nvidia-smi
   ```

3. Le container est lancÃ© avec `--gpus=all`
   ```bash
   docker ps  # VÃ©rifier les paramÃ¨tres du container
   ```

### ProblÃ¨me : Erreur de mÃ©moire GPU

**Solutions :**
- Augmenter `--shm-size` dans `devcontainer.json`
- Activer la croissance mÃ©moire (dÃ©jÃ  configurÃ© avec `TF_FORCE_GPU_ALLOW_GROWTH`)
- RÃ©duire la taille des batchs dans vos scripts

### ProblÃ¨me : Recompilation PTX (JIT) lente

**Solution :**
- Le cache GPU est montÃ© dans `~/.nv/ComputeCache`
- Assurez-vous que ce dossier existe sur votre machine hÃ´te
- La premiÃ¨re exÃ©cution peut Ãªtre lente, les suivantes utilisent le cache

### ProblÃ¨me : PEP 668 (pip install bloquÃ©)

**Solution :**
- âœ… **DÃ©jÃ  rÃ©solu** : Tous les packages sont installÃ©s via Conda
- N'utilisez jamais `pip install` en systÃ¨me, ajoutez les packages dans `environment.yml`

## ðŸ“ Ajouter des packages

### Via Conda (recommandÃ©)

Modifier `.devcontainer/environment.yml` :

```yaml
dependencies:
  - votre-package>=version
```

Puis reconstruire le container.

### Via pip (si non disponible sur conda)

Ajouter dans la section `pip:` de `environment.yml` :

```yaml
dependencies:
  - pip:
    - votre-package>=version
```

## ðŸ”„ Mettre Ã  jour l'environnement

Si vous modifiez `environment.yml` :

1. Reconstruire le container dans VS Code :
   - `F1` â†’ "Dev Containers: Rebuild Container"

2. Ou manuellement :
   ```bash
   docker build --no-cache -t gpu-dev-env .devcontainer/
   ```

## ðŸ“Š Monitoring GPU

### Avec nvidia-smi

```bash
# Monitoring en continu (toutes les 2 secondes)
watch -n 2 nvidia-smi

# Ou directement
nvidia-smi
```

### Avec PyTorch

```python
import torch
print(f"MÃ©moire allouÃ©e: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
print(f"MÃ©moire totale: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
```

### Avec TensorFlow

```python
import tensorflow as tf
gpus = tf.config.list_physical_devices('GPU')
for gpu in gpus:
    print(tf.config.experimental.get_memory_info(gpu.name))
```

## âœ… Checklist de validation

- [ ] `nvidia-smi` affiche le GPU RTX 5070
- [ ] `tf.config.list_physical_devices('GPU')` retourne au moins 1 GPU
- [ ] `torch.cuda.is_available()` retourne `True`
- [ ] Test de calcul matriciel rÃ©ussi sur GPU
- [ ] Jupyter Lab accessible sur http://localhost:8888
- [ ] Pas d'erreur PEP 668 (tout dans Conda)

## ðŸ“š Ressources

- [NVIDIA CUDA Documentation](https://docs.nvidia.com/cuda/)
- [TensorFlow GPU Support](https://www.tensorflow.org/install/gpu)
- [PyTorch CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html)
- [Dev Containers Documentation](https://code.visualstudio.com/docs/devcontainers/containers)

---

**Configuration testÃ©e avec :**
- GPU : NVIDIA RTX 5070
- CUDA : 12.8.0 + cuDNN
- TensorFlow : >= 2.20
- PyTorch : >= 2.5
- Python : 3.12
