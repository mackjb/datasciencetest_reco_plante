# TODO

JBM : JB
SL : Lionel
MP : Morgan
BG : Bernadette


dataframe √† la granularit√© Classe
- [X] Nombre de classification, images par classe pour plantvillage dans un df
- [SL] Nettoyer le graphique histograme pour lister les effectis par classe plantvillage
- [SL] Compl√©ter le df plantvillage avec les r√©solution et les extension (.jpg .png ...)

dataframe √† la granularit√© Images
- [MP] Parcourir toutes les images (segment√©es) et compter les pixel diff√©rentes du background (noir plantvillage, blanc flavia)
- [] Pourcentage des pixels non background sur la bordure 1 pixel de l'image 
- [] Choisir une m√©trique de d√©tection d'exposition de la photo, Ratio de pixels ‚Äúclipp√©s‚Äù sur une image, Moyenne et √©cart-type de la luminance, Entropie de l‚Äôhistogramme, M√©triques no-reference (NR-IQA)
- [JBM] Extraire les segments avec SAM et confronter les m√©triques plus haut 



üéØ Objectif g√©n√©ral du projet :

Utiliser le Machine Learning pour classifier efficacement des esp√®ces v√©g√©tales (Flavia) et d√©tecter des maladies (PlantVillage) √† partir d‚Äôimages, en r√©alisant une exploration rigoureuse des donn√©es, une visualisation pertinente, un pr√©traitement adapt√©, et un feature engineering efficace.

‚∏ª

üöß Backlog projet ML (Dataset Flavia & PlantVillage)

‚úÖ Partie 1 : Pr√©paration & Pr√©traitement des donn√©es
	‚Ä¢	1.1 Acquisition et structuration des donn√©es
	‚Ä¢	T√©l√©charger et v√©rifier les datasets Flavia et PlantVillage.
	‚Ä¢	Cr√©er une structure claire des dossiers (par esp√®ce et maladie).
	‚Ä¢	√âvaluer la qualit√© initiale des images (format, taille).
	‚Ä¢	1.2 Nettoyage et normalisation des images
	‚Ä¢	Redimensionnement uniforme √† 224x224 pixels.
	‚Ä¢	Filtrage Gaussien (sigma et noyau selon recommandations pr√©c√©dentes).
	‚Ä¢	Normalisation des pixels ([0,1]).
	‚Ä¢	1.3 Gestion des images probl√©matiques
	‚Ä¢	Identifier et supprimer des images corrompues ou non repr√©sentatives visuellement.
	‚Ä¢	V√©rifier visuellement les √©chantillons.
	‚Ä¢	1.4 Augmentation des donn√©es (Data Augmentation)
	‚Ä¢	Rotations, sym√©tries horizontales et verticales.
	‚Ä¢	L√©gers zooms et translations.
	‚Ä¢	Validation visuelle rapide.

‚∏ª

‚úÖ Partie 2 : Exploration des donn√©es (EDA)
	‚Ä¢	2.1 Analyse descriptive des datasets
	‚Ä¢	Nombre d‚Äôimages par esp√®ce/maladie.
	‚Ä¢	Identification de d√©s√©quilibres (classes majoritaires/minoritaires).
	‚Ä¢	2.2 Exploration statistique rapide des caract√©ristiques extraites
	‚Ä¢	Calculer moyennes, m√©dianes, variances des caract√©ristiques initiales (forme, couleur, texture).
	‚Ä¢	Identifier rapidement les √©ventuels NA (valeurs manquantes).
	‚Ä¢	2.3 Traitement des valeurs manquantes (NA) et aberrantes (Outliers)
	‚Ä¢	Imputation simple par m√©diane des NA.
	‚Ä¢	D√©tection des outliers (Isolation Forest recommand√©).
	‚Ä¢	Validation rapide par statistiques descriptives post-nettoyage.

‚∏ª

‚úÖ Partie 3 : Data Visualisation (EDA visuelle)
	‚Ä¢	3.1 Visualisation des distributions de classes
	‚Ä¢	Histogrammes / barplots du nombre d‚Äôimages par classe.
	‚Ä¢	3.2 Visualisation de caract√©ristiques cl√©s
	‚Ä¢	Boxplots par esp√®ce/maladie (Forme, Texture, Couleur).
	‚Ä¢	Identification visuelle rapide d‚Äôoutliers via boxplot.
	‚Ä¢	3.3 R√©duction de dimensionnalit√© (PCA / t-SNE) (optionnel mais recommand√©)
	‚Ä¢	PCA simple pour visualiser rapidement la s√©paration des esp√®ces et maladies en 2D.
	‚Ä¢	t-SNE si temps disponible pour une meilleure visualisation des groupes.

‚∏ª

‚úÖ Partie 4 : Feature Engineering
	‚Ä¢	4.1 Extraction des caract√©ristiques initiales (Baseline)
	‚Ä¢	Forme : Moments de Hu, Fourier descriptors, Solidity.
	‚Ä¢	Texture : GLCM (contraste, √©nergie, homog√©n√©it√©), Local Binary Patterns (LBP).
	‚Ä¢	Couleur : HSV (moyenne, std, histogrammes).
	‚Ä¢	Venation : Densit√© contours (Canny).
	‚Ä¢	4.2 Extraction des caract√©ristiques avanc√©es (am√©lioration)
	‚Ä¢	Forme : Eccentricit√©, ratio Aire/P√©rim√®tre.
	‚Ä¢	Texture : Filtres de Gabor.
	‚Ä¢	Couleur : Skewness, Kurtosis en HSV/LAB.
	‚Ä¢	Venation : √âventuellement Sobel/Laplacien avanc√©s.
	‚Ä¢	4.3 S√©lection des meilleures caract√©ristiques
	‚Ä¢	Utiliser Random Forest pour s√©lectionner les caract√©ristiques les plus importantes.
	‚Ä¢	V√©rifier les scores d‚Äôimportance des features.
	‚Ä¢	4.4 Validation du Feature Engineering
	‚Ä¢	Tester rapidement (10-fold cross-validation) avec SVM ou Random Forest pour comparer performances initiales avant optimisation.

‚∏ª

üö© Livrables finaux attendus √† ce stade du projet :

‚úîÔ∏è 1. Un rapport d‚ÄôEDA (Exploratory Data Analysis) clair avec des visualisations :
	‚Ä¢	Statistiques descriptives des datasets.
	‚Ä¢	Visualisations (boxplots, PCA, distributions).

‚úîÔ∏è 2. Un pipeline clair et r√©utilisable de pr√©traitement des donn√©es :
	‚Ä¢	Code document√© pour traitement NA et outliers.
	‚Ä¢	Code pour extraction compl√®te et standardis√©e des features.

‚úîÔ∏è 3. Une matrice finale des caract√©ristiques extraites pr√™te √† √™tre utilis√©e en mod√®le ML.