import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

def load_results():
    """Charge les r√©sultats des mod√®les"""
    base_dir = 'results/models/xgboost'
    
    # Charger les r√©sultats pour les maladies et les esp√®ces
    maladies = pd.read_csv(f'{base_dir}/nom_maladie/class_results.csv')
    especes = pd.read_csv(f'{base_dir}/nom_plante/class_results.csv')
    
    # Ajouter une colonne pour le type de donn√©es
    maladies['Type'] = 'Maladies'
    especes['Type'] = 'Esp√®ces'
    
    # Concat√©ner les r√©sultats
    return pd.concat([maladies, especes], ignore_index=True)

def plot_side_by_side_comparison(results_df):
    """Affiche une comparaison c√¥te √† c√¥te des m√©triques"""
    metrics = ['Precision', 'Recall', 'F1_score']
    fig, axes = plt.subplots(1, 3, figsize=(20, 6))
    
    for i, metric in enumerate(metrics):
        # Pr√©parer les donn√©es avec √©cart-type
        plot_data = results_df.groupby(['Type', 'Pipeline'])[metric].agg(['mean', 'std']).unstack()
        
        # Tracer avec barres d'erreur
        plot_data['mean'].plot(kind='bar', ax=axes[i], yerr=plot_data['std'], 
                             capsize=4, ecolor='black', alpha=0.8)
        
        axes[i].set_title(f'{metric} par mod√®le')
        axes[i].set_ylabel(metric)
        axes[i].set_ylim(0, 1.1)
        axes[i].grid(True, axis='y', linestyle='--', alpha=0.7)
        axes[i].legend(title='Mod√®le', bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.tight_layout()
    plt.savefig('figures/side_by_side_comparison.png', bbox_inches='tight')
    plt.close()

def plot_per_class_metrics(results_df, metric='F1_score'):
    """Affiche les m√©triques par classe pour chaque mod√®le"""
    for data_type in results_df['Type'].unique():
        plt.figure(figsize=(15, 8))
        
        # Filtrer les donn√©es
        df = results_df[results_df['Type'] == data_type]
        
        # Cr√©er un boxplot par mod√®le
        df.boxplot(column=metric, by=['Pipeline', 'Classe'], 
                  grid=False, rot=90, fontsize=8)
        
        plt.title(f'Distribution du {metric} par classe - {data_type}')
        plt.suptitle('')
        plt.xlabel('Mod√®le et Classe')
        plt.ylabel(metric)
        plt.tight_layout()
        
        # Sauvegarder le graphique
        plt.savefig(f'figures/per_class_{metric.lower()}_{data_type.lower()}.png', 
                   bbox_inches='tight')
        plt.close()

def analyze_overfitting(results_df):
    """Analyse le surapprentissage √† partir des r√©sultats"""
    # Calculer les moyennes par mod√®le et type
    metrics = ['Precision', 'Recall', 'F1_score']
    
    # Pr√©parer les r√©sultats
    analysis = {}
    
    for (model_type, pipeline), group in results_df.groupby(['Type', 'Pipeline']):
        if pipeline not in analysis:
            analysis[pipeline] = {}
        
        # Calculer les m√©triques moyennes et √©cart-type
        analysis[pipeline][model_type] = {
            'mean': {
                metric: group[metric].mean() * 100 
                for metric in metrics
            },
            'std': {
                metric: group[metric].std() * 100
                for metric in metrics
            },
            'min': {
                metric: group[metric].min() * 100
                for metric in metrics
            },
            'max': {
                metric: group[metric].max() * 100
                for metric in metrics
            },
            'n_classes': group['Classe'].nunique()
        }
    
    return analysis

def print_analysis(analysis):
    """Affiche l'analyse du surapprentissage"""
    print("\n" + "="*100)
    print("ANALYSE D√âTAILL√âE DES PERFORMANCES ET DU SURAPPRENTISSAGE")
    print("="*100)
    
    for pipeline, data in analysis.items():
        print(f"\n\n{'='*50}")
        print(f"MOD√àLE: {pipeline}")
        print(f"{'='*50}")
        
        for model_type, metrics in data.items():
            print(f"\nüîç {model_type.upper()}:")
            print(f"   {'M√©trique':<15} {'Moyenne':<10} {'√âcart-type':<12} {'Min':<10} {'Max':<10}")
            print(f"   {'-'*15} {'-'*10} {'-'*12} {'-'*10} {'-'*10}")
            
            for metric in ['Precision', 'Recall', 'F1_score']:
                mean = metrics['mean'][metric]
                std = metrics['std'][metric]
                min_val = metrics['min'][metric]
                max_val = metrics['max'][metric]
                
                # D√©terminer l'ic√¥ne en fonction de la variance
                if std > 15:
                    icon = "‚ö†Ô∏è"
                elif std < 5:
                    icon = "‚úÖ"
                else:
                    icon = "‚ÑπÔ∏è"
                
                print(f"   {icon} {metric:<12} {mean:>6.2f}% ¬±{std:>5.2f}%  {min_val:>6.2f}%  {max_val:>6.2f}%")
            
            # Afficher l'interpr√©tation
            print("\n   INTERPR√âTATION:")
            if metrics['std']['F1_score'] > 15:
                print("   ‚ö†Ô∏è  Forte variance entre les classes - Risque de surapprentissage sur certaines classes")
            elif metrics['std']['F1_score'] < 5:
                print("   ‚úÖ  Faible variance - Bonne coh√©rence entre les classes")
            else:
                print("   ‚ÑπÔ∏è  Variance mod√©r√©e - Certaines classes peuvent n√©cessiter plus d'attention")
            
            # V√©rifier l'√©cart min-max
            f1_range = metrics['max']['F1_score'] - metrics['min']['F1_score']
            if f1_range > 40:
                print(f"   ‚ö†Ô∏è  Grand √©cart de performance entre classes ({f1_range:.1f}%) - V√©rifier les classes probl√©matiques")
            
            print(f"\n   Nombre de classes: {metrics['n_classes']}")

def main():
    print("Chargement et pr√©paration des r√©sultats...")
    results_df = load_results()
    
    # Nettoyer les donn√©es
    results_df = results_df[results_df['Config'] == 'Baseline']
    
    print("\nAnalyse des performances...")
    analysis = analyze_overfitting(results_df)
    
    # Cr√©er le dossier pour les figures
    os.makedirs('figures', exist_ok=True)
    
    print("\nG√©n√©ration des visualisations...")
    # Graphique comparatif c√¥te √† c√¥te
    plot_side_by_side_comparison(results_df)
    
    # Graphiques par classe
    for metric in ['F1_score', 'Precision', 'Recall']:
        plot_per_class_metrics(results_df, metric)
    
    # Afficher l'analyse d√©taill√©e
    print("\n" + "="*100)
    print("R√âSULTATS D√âTAILL√âS PAR MOD√àLE")
    print("="*100)
    print_analysis(analysis)
    
    print("\n" + "="*100)
    print("ANALYSE TERMIN√âE")
    print("="*100)
    print("Les graphiques ont √©t√© sauvegard√©s dans le dossier 'figures/'")
    print("\nR√âCAPITULATIF DES PRINCIPAUX POINTS :")
    print("-" * 50)
    print("1. Comparaison des mod√®les : Voir 'figures/side_by_side_comparison.png'")
    print("2. D√©tail par classe : Voir les fichiers 'per_class_*_*.png'")
    print("3. Analyse compl√®te : Voir le rapport ci-dessus pour les d√©tails par mod√®le")

if __name__ == "__main__":
    main()
