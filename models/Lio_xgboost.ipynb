{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5dcd935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå± Colonnes plantes d√©tect√©es : ['plant_Apple', 'plant_Blueberry', 'plant_Cherry_(including_sour)', 'plant_Corn_(maize)', 'plant_Grape', 'plant_Orange', 'plant_Peach', 'plant_Pepper,_bell', 'plant_Potato', 'plant_Raspberry', 'plant_Soybean', 'plant_Squash', 'plant_Strawberry', 'plant_Tomato']\n",
      "‚úÖ Apr√®s suppression des NaN : 54275 √©chantillons restants.\n",
      "‚úÖ Dataset pr√™t : 54275 √©chantillons, 38 features num√©riques.\n",
      "Nombre de classes (plantes) : 14\n",
      "\n",
      "üöÄ Test avec StandardScaler + Config = Baseline\n",
      "   Fold 1 : F1 = 0.9988\n",
      "   Fold 2 : F1 = 0.9990\n",
      "   Fold 3 : F1 = 0.9979\n",
      "   Fold 4 : F1 = 0.9993\n",
      "   Fold 5 : F1 = 0.9982\n",
      "   ‚û°Ô∏è Moyenne F1 CV = 0.9986 ¬± 0.0005\n",
      "üìä Test final | Accuracy = 0.9983, F1 = 0.9983, ROC-AUC = 0.9991\n",
      "\n",
      "üöÄ Test avec StandardScaler + Config = Deep Trees\n",
      "   Fold 1 : F1 = 0.9985\n",
      "   Fold 2 : F1 = 0.9987\n",
      "   Fold 3 : F1 = 0.9982\n",
      "   Fold 4 : F1 = 0.9993\n",
      "   Fold 5 : F1 = 0.9982\n",
      "   ‚û°Ô∏è Moyenne F1 CV = 0.9986 ¬± 0.0004\n",
      "üìä Test final | Accuracy = 0.9982, F1 = 0.9983, ROC-AUC = 0.9991\n",
      "\n",
      "üöÄ Test avec StandardScaler + Config = Shallow Trees\n",
      "   Fold 1 : F1 = 0.9969\n",
      "   Fold 2 : F1 = 0.9972\n",
      "   Fold 3 : F1 = 0.9967\n",
      "   Fold 4 : F1 = 0.9961\n",
      "   Fold 5 : F1 = 0.9963\n",
      "   ‚û°Ô∏è Moyenne F1 CV = 0.9966 ¬± 0.0004\n",
      "üìä Test final | Accuracy = 0.9965, F1 = 0.9965, ROC-AUC = 0.9981\n",
      "\n",
      "üìä Tableau comparatif des r√©sultats :\n",
      "           Scaler         Config  CV_F1_mean  CV_F1_std  Test_Accuracy  \\\n",
      "0  StandardScaler       Baseline    0.998642   0.000516       0.998342   \n",
      "1  StandardScaler     Deep Trees    0.998573   0.000428       0.998250   \n",
      "2  StandardScaler  Shallow Trees    0.996649   0.000405       0.996499   \n",
      "\n",
      "    Test_F1  Test_AUC  \n",
      "0  0.998345  0.999111  \n",
      "1  0.998253  0.999062  \n",
      "2  0.996512  0.998123  \n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# IMPORT DES LIBRAIRIES\n",
    "# ------------------------------------------------------\n",
    "\n",
    "import pandas as pd               # manipulation de tableaux (DataFrame)\n",
    "import numpy as np                # op√©rations math√©matiques et tableaux\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold  # split dataset + validation crois√©e\n",
    "from sklearn.preprocessing import StandardScaler  # normalisation\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score  # m√©triques d'√©valuation\n",
    "from xgboost import XGBClassifier  # mod√®le XGBoost (arbres boost√©s)\n",
    "from imblearn.over_sampling import SMOTE  # pour r√©√©quilibrer les classes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # ignorer les warnings pour un affichage clair\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------------------------------\n",
    "\n",
    "FEATURES_PATH = \"/workspaces/datasciencetest_reco_plante/dataset/plantvillage/csv/clean_with_features_data_plantvillage_segmented_all.csv\"\n",
    "SEED = 42  # graine al√©atoire pour reproductibilit√©\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# D√âTECTION AUTOMATIQUE DES COLONNES PLANTES (labels)\n",
    "# ------------------------------------------------------\n",
    "plant_columns = [col for col in pd.read_csv(FEATURES_PATH, nrows=1).columns if col.startswith(\"plant_\")]\n",
    "print(f\"üå± Colonnes plantes d√©tect√©es : {plant_columns}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# CHARGEMENT DU DATASET\n",
    "# ------------------------------------------------------\n",
    "df = pd.read_csv(FEATURES_PATH)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# SUPPRESSION DES VALEURS MANQUANTES (NaN)\n",
    "# ------------------------------------------------------\n",
    "numeric_columns = df.drop(columns=plant_columns).select_dtypes(include=np.number).columns\n",
    "df = df.dropna(subset=numeric_columns.tolist() + plant_columns)\n",
    "print(f\"‚úÖ Apr√®s suppression des NaN : {df.shape[0]} √©chantillons restants.\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# PR√âPARATION DES DONN√âES\n",
    "# ------------------------------------------------------\n",
    "X = df[numeric_columns].values  # features num√©riques\n",
    "y = df[plant_columns].values    # labels multi-classes (one-hot)\n",
    "y = np.argmax(y, axis=1)        # transforme one-hot en labels entiers (0,1,...,13)\n",
    "print(f\"‚úÖ Dataset pr√™t : {X.shape[0]} √©chantillons, {X.shape[1]} features num√©riques.\")\n",
    "print(f\"Nombre de classes (plantes) : {len(plant_columns)}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# NORMALISATION\n",
    "# ------------------------------------------------------\n",
    "scalers = {\"StandardScaler\": StandardScaler()}  # moyenne=0, variance=1\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# CONFIGURATIONS XGBOOST\n",
    "# ------------------------------------------------------\n",
    "configs = {\n",
    "    \"Baseline\": {\"n_estimators\": 200, \"learning_rate\": 0.1, \"max_depth\": 6},\n",
    "    \"Deep Trees\": {\"n_estimators\": 300, \"learning_rate\": 0.05, \"max_depth\": 10},\n",
    "    \"Shallow Trees\": {\"n_estimators\": 500, \"learning_rate\": 0.01, \"max_depth\": 3}\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# FONCTION D'√âVALUATION\n",
    "# ------------------------------------------------------\n",
    "def evaluate(y_true, y_pred, dataset_name=\"Test\"):\n",
    "    \"\"\"\n",
    "    Calcule Accuracy, F1-score pond√©r√© et ROC-AUC pond√©r√© si possible.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    try:\n",
    "        auc = roc_auc_score(pd.get_dummies(y_true), pd.get_dummies(y_pred), average=\"weighted\")\n",
    "    except:\n",
    "        auc = None\n",
    "    print(f\"üìä {dataset_name} | Accuracy = {acc:.4f}, F1 = {f1:.4f}\", end=\"\")\n",
    "    if auc is not None:\n",
    "        print(f\", ROC-AUC = {auc:.4f}\")\n",
    "    else:\n",
    "        print(\" (ROC-AUC non calculable - multi-classes)\")\n",
    "    return acc, f1, auc\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# BOUCLE PRINCIPALE : SCALERS + CONFIGS + VALIDATION CROIS√âE\n",
    "# ------------------------------------------------------\n",
    "results = []\n",
    "\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "    )\n",
    "\n",
    "    for config_name, params in configs.items():\n",
    "        print(f\"\\nüöÄ Test avec {scaler_name} + Config = {config_name}\")\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"mlogloss\",\n",
    "            random_state=SEED,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        # Validation crois√©e 5-fold stratifi√©e\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        f1_scores = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "            X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            # SMOTE pour r√©√©quilibrer les classes minoritaires\n",
    "            smote = SMOTE(random_state=SEED)\n",
    "            X_tr_bal, y_tr_bal = smote.fit_resample(X_tr, y_tr)\n",
    "\n",
    "            # Entra√Ænement du mod√®le\n",
    "            model.fit(X_tr_bal, y_tr_bal)\n",
    "\n",
    "            # Pr√©diction sur le fold de validation\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            f1_fold = f1_score(y_val, y_val_pred, average=\"weighted\")\n",
    "            f1_scores.append(f1_fold)\n",
    "            print(f\"   Fold {fold+1} : F1 = {f1_fold:.4f}\")\n",
    "\n",
    "        print(f\"   ‚û°Ô∏è Moyenne F1 CV = {np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\")\n",
    "\n",
    "        # R√©entra√Ænement sur tout le train avec SMOTE\n",
    "        smote = SMOTE(random_state=SEED)\n",
    "        X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "        model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "        # √âvaluation finale sur le test set\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        acc, f1, auc = evaluate(y_test, y_test_pred, dataset_name=\"Test final\")\n",
    "\n",
    "        # Stockage des r√©sultats\n",
    "        results.append({\n",
    "            \"Scaler\": scaler_name,\n",
    "            \"Config\": config_name,\n",
    "            \"CV_F1_mean\": np.mean(f1_scores),\n",
    "            \"CV_F1_std\": np.std(f1_scores),\n",
    "            \"Test_Accuracy\": acc,\n",
    "            \"Test_F1\": f1,\n",
    "            \"Test_AUC\": auc\n",
    "        })\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# AFFICHAGE DES R√âSULTATS FINAUX\n",
    "# ------------------------------------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nüìä Tableau comparatif des r√©sultats :\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
