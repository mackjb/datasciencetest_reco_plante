import pandas as pd
import numpy as np
from pycaret.classification import *
import logging
import os
from datetime import datetime

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("automl_test.log"),
        logging.StreamHandler()
    ]
)

def main():
    try:
        # Charger un plus grand sous-ensemble de donn√©es
        logging.info("Chargement des donn√©es...")
        df = pd.read_csv('dataset/plantvillage/csv/clean_data_plantvillage_segmented_all_with_features.csv')
        
        # Prendre un √©chantillon plus petit pour acc√©l√©rer les tests
        sample = df.sample(2000, random_state=42)  # R√©duit √† 2000 √©chantillons pour les tests
        
        # Pr√©parer les donn√©es
        features = [col for col in sample.select_dtypes(include=np.number).columns 
                  if col not in ['nom_plante', 'nom_maladie', 'Est_Saine']]
        
        data = sample[features + ['nom_plante']].copy()
        data = data.rename(columns={'nom_plante': 'target'})
        
        # Cr√©er le dossier de sortie
        os.makedirs('results/test_models', exist_ok=True)
        
        # Configuration simplifi√©e de PyCaret pour acc√©l√©rer les tests
        logging.info("Configuration de PyCaret...")
        exp = setup(
            data=data,
            target='target',
            train_size=0.8,
            session_id=42,
            normalize=True,  # Normalisation de base
            normalize_method='zscore',  # Plus rapide que 'robust'
            feature_selection=False,  # D√©sactiver la s√©lection de caract√©ristiques
            remove_multicollinearity=False,  # D√©sactiver la suppression de la multicolin√©arit√©
            fix_imbalance=False,  # D√©sactiver la gestion du d√©s√©quilibre
            fold_strategy='kfold',  # Plus rapide que 'stratifiedkfold'
            fold=2,  # R√©duire le nombre de folds
            verbose=True,
            log_experiment=False,
            experiment_name=f'test_models_{datetime.now().strftime("%Y%m%d_%H%M")}'
        )
        
        # Mod√®les les plus rapides pour les tests initiaux
        models_to_compare = [
            'lr',  # R√©gression logistique (le plus rapide)
            'dt'   # Arbre de d√©cision (rapide et interpr√©table)
        ]
        
        # Comparaison des mod√®les
        logging.info("Comparaison des mod√®les...")
        best_models = compare_models(
            include=models_to_compare,
            n_select=1,  # S√©lectionner uniquement le meilleur mod√®le
            sort='Accuracy',  # Trier par pr√©cision pour la simplicit√©
            fold=2,      # 2-fold cross-validation pour acc√©l√©rer
            round=4,     # 4 d√©cimales pour les m√©triques
            verbose=True
        )
        
        # Sauvegarder les meilleurs mod√®les
        for i, model in enumerate(best_models):
            model_name = model.__class__.__name__
            save_model(model, f'results/test_models/best_model_{i+1}_{model_name}')
            logging.info(f"Mod√®le {i+1} sauvegard√©: {model_name}")
        
        # Afficher le meilleur mod√®le
        best_model = best_models[0]
        logging.info(f"\nüéâ Meilleur mod√®le: {best_model.__class__.__name__}")
        
        # √âvaluation du meilleur mod√®le
        logging.info("\nüìä √âvaluation du meilleur mod√®le...")
        evaluate_model(best_model)
        
        # Pr√©dictions sur l'ensemble de test
        logging.info("\nüîÆ G√©n√©ration des pr√©dictions...")
        predictions = predict_model(best_model)
        
        # Sauvegarder les pr√©dictions
        predictions.to_csv('results/test_models/predictions.csv', index=False)
        logging.info("Pr√©dictions sauvegard√©es dans results/test_models/predictions.csv")
        
    except Exception as e:
        logging.error(f"\n‚ùå Erreur: {str(e)}", exc_info=True)
        raise

if __name__ == "__main__":
    main()
